\section{Lesson 18}

\subsection{Notes}
\begin{enumerate}
    \item Recall that the eigenvectors for unique eigenvalues are linearly independent.
    \item The rational canonical form of diagonalization avoids using complex numbers. Recall that eigenvalues and their eigenvectors come in complex conjugate pairs:
          \begin{align*}
              \lambda_\pm    & =\alpha\pm\beta i       \\
              \mathbf{v}_\pm & =\mathbf{x}+i\mathbf{y}
          \end{align*}
          The rational canonical form of the diagonalization of $A$ can be written as
          \begin{align*}
              A=\left[\begin{matrix}\mathbf{x} & \mathbf{y}\end{matrix}\right]\left[\begin{matrix}\alpha&\beta\cr-\beta&\alpha\end{matrix}\right][\begin{matrix}\mathbf{x} & \mathbf{y}\end{matrix}]^{-1}\text{.}
          \end{align*}
    \item \textbf{Scalar multiples}:
          \begin{align*}
              \alpha A = \alpha \, SDS^{-1}=S(\alpha D)S^{-1}
          \end{align*}
          That is, $\alpha A$ is also diagonalizable with the same eigenvectors but each eigenvalue scaled by $\alpha$.
    \item \textbf{Shifts}:
          \begin{align*}
              A+\alpha I ~=~SDS^{-1}+\alpha SIS^{-1}~=~S(D+\alpha I)S^{-1}
          \end{align*}
          That is, $A+\alpha I$ is also diagonalizable with the same eigenvectors but $\alpha$ added to each eigenvalue.
    \item \textbf{Powers}:
          \begin{align*}
              A^p ~=~(SDS^{-1})^p~=~SD^pS^{-1}
          \end{align*}
          That is, $A^p$ (when it is defined) is also diagonalizable with the same eigenvectors but each eigenvalue raised to $p$. This is valid for negative powers as well.
    \item \textbf{Polynomial functions}:
          \begin{align*}
              p(A) = S\,p(D)\,S^{-1}
          \end{align*}
          That is, $p(A)$ is also diagonalizable with the same eigenvectors but $p$ applied to each eigenvalue.
    \item If $\text{col}(A)\cap\text{nul}(A)$ is nontrivial, then $A$ is defective.
    \item Diagonalization $A=SDS^-1$ can be thought of geometrically.
          \begin{align*}
              \begin{matrix}
                          &                                   & 4               &                                 & \\
                          &                                   &                 &                                 & \\
                          & \mathbf{x}                        & \longrightarrow & A\mathbf{x}=S\mathbf{\beta}     & \\
                  1\qquad & \downarrow                        &                 & \uparrow                        &
                  \qquad 3\cr
                          & \mathbf{\alpha}=S^{-1} \mathbf{x} & \longrightarrow & \mathbf{\beta}=D\mathbf{\alpha} & \\
                          &                                   &                 &                                 & \\
                          &                                   & 2                                                   \\
              \end{matrix}
          \end{align*}
          \begin{align*}
              1 & : \text{change of coordinates to the eigenbasis}        \\
              2 & : \text{action of A with respect to the eigenbasis}     \\
              3 & : \text{hange of coordinates to the standard basis}     \\
              4 & : \text{action of A with respect to the standard basis}
          \end{align*}
    \item Two matrices $A_1$ and $A_2$ are \textbf{simultaneously diagonalizable} if
          \begin{align*}
              A_1 & = SD_1S^{-1}\text{,} \\
              A_2 & = SD_2S^{-1}\text{.}
          \end{align*}
          That is, both matrices have the same eigenvectors but not necessarily the same eigenvalues.
    \item Simultaneously diagonalizable matrices commute.
    \item An \textbf{eigenspace} associated with the matrix $A$ is denoted by
          \begin{align*}
              E_\lambda(A)=\text{nul}(A-\lambda I)\text{.}
          \end{align*}
          \begin{enumerate}
              \item Every eigenvector of $A$ with eigenvalue $\lambda$ is in $E_\lambda(A)$.
              \item Every nonzero vector in $E_\lambda(A)$ is an eigenvector of $A$ with eigenvalue $\lambda$.
              \item The intersection of eigenspaces corresponding to different eigenvalues is the trivial subspace.
              \item If $A$ is a diagonalizable matrix, then the direct sum of its eigenspaces is $\mathbb{R}^n$.
          \end{enumerate}
    \item (Follow Up Problem 18.10) Two diagonalizations of the same matrix must share the same eigenvalues and eigenspaces, but there is substantial freedom in choosing the eigenbasis. That is, for an $n\times n$ matrix $A$, there are $n$ matrices possible for $D$ (changing the order of the eigenvalues) but many more possible matrices $S$ (as long as $\text{col}(S)=E_\lambda(A)$), such that $A=SDS^{-1}$.
    \item (Follow Up Problem 18.14) When the multiplicity of an eigenvalue is 2 or more, the square root of the diagonal matrix need not be diagonal, e.g.
          \begin{align*}
              \left[\begin{matrix}
                            0 & 1 \\
                            1 & 0
                        \end{matrix}\right]
              \left[\begin{matrix}
                            0 & 1 \\
                            1 & 0
                        \end{matrix}\right]
              =
              \left[\begin{matrix}
                            1 & 0 \\
                            0 & 1
                        \end{matrix}\right]
          \end{align*}
    \item (Follow Up Problems 19.1-19.3) \textbf{Spectral decomposition}:
          \begin{align*}
              A = SDS^{-1} & =
              \left[\begin{matrix}
                            \mathbf{v}_1 & \cdots & \mathbf{v}_n
                        \end{matrix}\right]
              \left[\begin{matrix}
                            \lambda_1 &        &           \\
                                      & \ddots &           \\
                                      &        & \lambda_n
                        \end{matrix}\right]
              \left[\begin{matrix}
                            \mathbf{u}^T_1 \\
                            \vdots         \\
                            \mathbf{u}^T_n
                        \end{matrix}\right]                            \\
                           & = \lambda_1\mathbf{v}_1\mathbf{u}^T_1 + \cdots + \lambda_n\mathbf{v}_n\mathbf{u}^T_n\text{,}
          \end{align*}
          where each $\mathbf{v}_\ell$ is a column from $S$, and each $\mathbf{u}^T_\ell$ is a row from $S^{-1}$. 

\end{enumerate}